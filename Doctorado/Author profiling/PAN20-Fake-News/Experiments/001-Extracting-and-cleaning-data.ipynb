{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and cleaning data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../Scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.3.0/es_core_news_sm-3.3.0-py3-none-any.whl#egg=es_core_news_sm==3.3.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.3.0/es_core_news_sm-3.3.0-py3-none-any.whl (12.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 14.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from es-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.22.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (56.0.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.1.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->es-core-news-sm==3.3.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json, xmltodict, os\n",
    "import text_preprocessing_v3 as tpv3\n",
    "import numpy as np, pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data paths\n",
    "path_train_en = '../Datasets/pan20-author-profiling-training-2020-02-23/en/'\n",
    "path_train_es = '../Datasets/pan20-author-profiling-training-2020-02-23/es/'\n",
    "path_train_labels_en = '../Datasets/pan20-author-profiling-training-2020-02-23/en.txt'\n",
    "path_train_labels_es = '../Datasets/pan20-author-profiling-training-2020-02-23/es.txt'\n",
    "\n",
    "# Testing data paths\n",
    "path_test_en = '../Datasets/pan20-author-profiling-test-2020-02-23/en/'\n",
    "path_test_es = '../Datasets/pan20-author-profiling-test-2020-02-23/es/'\n",
    "path_test_labels_en = '../Datasets/pan20-author-profiling-test-2020-02-23/en.txt'\n",
    "path_test_labels_es = '../Datasets/pan20-author-profiling-test-2020-02-23/es.txt'\n",
    "\n",
    "# Saving data path\n",
    "path_to_save = '../Datasets/CSV/'\n",
    "path_to_save_en = '../Datasets/JSON/en/'\n",
    "path_to_save_es = '../Datasets/JSON/es/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_names(path):\n",
    "\n",
    "    file_names = []\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        f = os.path.join(path, filename)\n",
    "\n",
    "        if os.path.isfile(f):\n",
    "            if '.txt' not in filename:\n",
    "                file_names.append(filename.split('.')[0])\n",
    "    \n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining all the file names \n",
    "\n",
    "# Data train\n",
    "file_names_training_en = get_list_names(path_train_en)\n",
    "file_names_training_es = get_list_names(path_train_es)\n",
    "\n",
    "# Data test\n",
    "file_names_test_en = get_list_names(path_test_en)\n",
    "file_names_test_es = get_list_names(path_test_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data\n",
      "Training --> (EN): 300, (ES): 300\n",
      "Test --> (EN): 200, (ES): 200\n"
     ]
    }
   ],
   "source": [
    "print('Total data')\n",
    "print(f'Training --> (EN): {len(file_names_training_en)}, (ES): {len(file_names_training_es)}')\n",
    "print(f'Test --> (EN): {len(file_names_test_en)}, (ES): {len(file_names_test_es)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check file names \n",
    "def check_path(file_name):\n",
    "    if file_name[0].isdigit():\n",
    "        # return os.path.join(f'\\', file_name)\n",
    "        return file_name\n",
    "    else:\n",
    "        return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_json(path, file_names, path_to_save):\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        file_name = check_path(file_name)\n",
    "        final_path = f'{path}{file_name}.xml'\n",
    "\n",
    "        # Reading XML file and converting it into text\n",
    "        with open(final_path, encoding = 'utf-8') as xml_file:\n",
    "            data = xmltodict.parse(xml_file.read())\n",
    "        \n",
    "        # Converting text into JSON style\n",
    "        data = json.dumps(data, indent = 2)\n",
    "\n",
    "        # Saving JSON file\n",
    "        saving_path = f'{path_to_save}{file_name}.json'\n",
    "        with open(saving_path, 'w') as json_file:\n",
    "            json_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_to_json(path_train_en, file_names_training_en, f'{path_to_save_en}train/')\n",
    "xml_to_json(path_train_es, file_names_training_es, f'{path_to_save_es}train/')\n",
    "xml_to_json(path_test_en, file_names_test_en, f'{path_to_save_en}test/')\n",
    "xml_to_json(path_test_es, file_names_test_es, f'{path_to_save_es}test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels_path, file_name):\n",
    "    # Leyendo y creando un DF de las etiquetas del archivo JSON\n",
    "    df_labels = pd.read_csv(labels_path, sep = ':::', names = ['id', 'label'], \n",
    "                        engine = 'python')\n",
    "    \n",
    "    labels = df_labels[df_labels.id == file_name].label\n",
    "\n",
    "    return labels.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(path, labels_path, file_names):\n",
    "    data = pd.DataFrame(columns = ['id', 'tweet', 'label'])\n",
    "\n",
    "    for file_name in file_names:\n",
    "        # Verificar PATH\n",
    "        file_name = check_path(file_name)\n",
    "        \n",
    "        # Leer archivo JSON basado en el nombre del archivo\n",
    "        with open(f'{path}{file_name}.json') as f:\n",
    "            json_file = json.load(f)\n",
    "        \n",
    "        # Obteniendo la cantidad de documentos que tiene el archivo JSON\n",
    "        file_len = len(json_file['author']['documents']['document'])\n",
    "\n",
    "        # Obtner las etiquetas del JSON\n",
    "        labels = get_labels(labels_path, file_name)\n",
    "\n",
    "        dict = {'id': [file_name] * file_len,\n",
    "                'tweet': json_file['author']['documents']['document'],\n",
    "                'label': [labels] * file_len}\n",
    "\n",
    "        df = pd.DataFrame(data = dict)\n",
    "        data = data.append(df, ignore_index=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_paths = [f'{path_to_save_en}train/', f'{path_to_save_es}train/', f'{path_to_save_en}test/', f'{path_to_save_es}test/']\n",
    "labels = [path_train_labels_en, path_train_labels_es, path_test_labels_en, path_test_labels_es]\n",
    "filenames = [file_names_training_en, file_names_training_es, file_names_test_en, file_names_test_es]\n",
    "json_path_names = ['Train-EN', 'Train-ES', 'Test-EN', 'Test-ES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV data: 30000\n",
      "Total CSV data: 30000\n",
      "Total CSV data: 20000\n",
      "Total CSV data: 20000\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    data = json_to_csv(json_paths[i], labels[i], filenames[i])\n",
    "    data.to_csv(f'{path_to_save}{json_path_names[i]}.csv', encoding='utf-8')\n",
    "    print(f'Total CSV data: {data.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '../Datasets/CSV/'\n",
    "data_training_en_path = f'{main_path}Train-EN.csv'\n",
    "data_training_es_path = f'{main_path}Train-ES.csv'\n",
    "data_test_en_path = f'{main_path}Test-EN.csv'\n",
    "data_test_es_path = f'{main_path}Test-ES.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "data_training_en = pd.read_csv(data_training_en_path)\n",
    "data_test_en = pd.read_csv(data_test_en_path)\n",
    "\n",
    "# Spanish\n",
    "data_training_es = pd.read_csv(data_training_es_path)\n",
    "data_test_es = pd.read_csv(data_test_es_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data v3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_en = tpv3.Preprocessing(language='english')\n",
    "prep_es = tpv3.Preprocessing(language='spanish')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_en = prep_en.main_preprocess(data=data_training_en, \n",
    "                                column='tweet', \n",
    "                                remove_stop_words=False, \n",
    "                                is_dataframe=True,\n",
    "                                lemmatize=True, \n",
    "                                emoji_path=None,\n",
    "                                tweet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_en = prep_en.main_preprocess(data=data_test_en, \n",
    "                                column='tweet', \n",
    "                                remove_stop_words=False, \n",
    "                                is_dataframe=True,\n",
    "                                lemmatize=True, \n",
    "                                emoji_path=None,\n",
    "                                tweet=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_es = prep_es.main_preprocess(data=data_training_es, \n",
    "                                column='tweet', \n",
    "                                remove_stop_words=False, \n",
    "                                is_dataframe=True,\n",
    "                                lemmatize=True, \n",
    "                                emoji_path=None,\n",
    "                                tweet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_es = prep_es.main_preprocess(data=data_test_es, \n",
    "                                column='tweet', \n",
    "                                remove_stop_words=False, \n",
    "                                is_dataframe=True,\n",
    "                                lemmatize=True, \n",
    "                                emoji_path=None,\n",
    "                                tweet=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data_training_en, data_test_en, data_training_es, data_test_es]\n",
    "data_names = ['data_training_en_lemma', 'data_test_en_lemma', 'data_training_es_lemma', 'data_test_es_lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    data[i].to_csv(f'{main_path}Clean/{data_names[i]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
