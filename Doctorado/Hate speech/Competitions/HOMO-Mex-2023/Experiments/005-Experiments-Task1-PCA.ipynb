{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'text-preprocessing' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/CCogS-Mx/text-preprocessing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('Scripts/')\n",
    "sys.path.append('text-preprocessing/script/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import classes\n",
    "import text_preprocessing as tp\n",
    "import feature_extraction as fe\n",
    "import robust_classic_model as rcm\n",
    "import cleaning_twitter_data as ctd\n",
    "\n",
    "# Algoritmos de ML\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vectorizadores\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('Data/homomex_training.csv').drop('index', axis=1)\n",
    "test = pd.read_csv('Data/track1_test_no_labels.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = tp.Preprocessing('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_p = prep.main_preprocess(data=training, \n",
    "                                  column='tweets', \n",
    "                                  tweet=True, \n",
    "                                  tweet_tags=True, \n",
    "                                  remove_stop_words=False, \n",
    "                                  lemmatize=False, \n",
    "                                  translate_emojis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_p.to_csv('Data/task1_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_p.label = training_p.label.fillna('NR')\n",
    "dictionary_list = [{'P': 0, 'NP': 1, 'NR': 2}]\n",
    "training_p.label.replace(list(dictionary_list[0].keys()), \n",
    "            list(dictionary_list[0].values()), \n",
    "            inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p = prep.main_preprocess(data=test, \n",
    "                                  column='content', \n",
    "                                  tweet=True, \n",
    "                                  tweet_tags=True, \n",
    "                                  remove_stop_words=False, \n",
    "                                  lemmatize=False, \n",
    "                                  translate_emojis=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving best classifiers with training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (1,1), lowercase = False)\n",
    "oth_feats_vectorizer = CountVectorizer(ngram_range = (1,1), lowercase = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_p, \n",
    "                                                    training_p['label'], \n",
    "                                                    test_size=0.05, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "f_train = fe.FeatureExtraction(data = X_train, \n",
    "                                text_column = 'tweets', \n",
    "                                lemma = True, \n",
    "                                pos = True, \n",
    "                                tag = True,                                \n",
    "                                other_features = True,\n",
    "                                vectorizer = vectorizer,\n",
    "                                oth_feats_vectorizer = oth_feats_vectorizer,\n",
    "                                language = 'es')\n",
    "\n",
    "X = f_train.add_features_to_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "f_train = fe.FeatureExtraction(data = X_test, \n",
    "                                text_column = 'tweets', \n",
    "                                lemma = True, \n",
    "                                pos = True, \n",
    "                                tag = True,                                \n",
    "                                other_features = True,\n",
    "                                vectorizer = vectorizer,\n",
    "                                oth_feats_vectorizer = oth_feats_vectorizer,\n",
    "                                language = 'es')\n",
    "\n",
    "x = f_train.add_features_to_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_count = [LogisticRegression(C=1.0, max_iter=1000, penalty='l2', random_state=42), \n",
    "          RandomForestClassifier(max_depth=50, n_estimators=10,random_state=42), \n",
    "          CalibratedClassifierCV(LinearSVC(C=0.1, max_iter=100000, penalty='l2', random_state=42)), \n",
    "          KNeighborsClassifier(n_neighbors=3, weights='distance')]\n",
    "models_tfidf = [LogisticRegression(C=10.0, max_iter=1000, penalty='l2', random_state=42), \n",
    "          RandomForestClassifier(max_depth=100, n_estimators=10,random_state=42), \n",
    "          CalibratedClassifierCV(LinearSVC(C=1.0, max_iter=1000, penalty='l2', random_state=42)), \n",
    "          KNeighborsClassifier(n_neighbors=3, weights='distance')]\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'Linear SVC', 'KNN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1000, random_state=42)\n",
    "vectorizers = [CountVectorizer(ngram_range = (1,1), lowercase = False), \n",
    "              TfidfVectorizer(ngram_range = (1,1), lowercase = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "Model: Logistic Regression, f1: 0.709\n",
      "Model: Random Forest, f1: 0.3876\n",
      "Model: Linear SVC, f1: 0.6907\n",
      "Model: KNN, f1: 0.4101\n",
      "TF-IDF Vectorizer\n",
      "Model: Logistic Regression, f1: 0.6455\n",
      "Model: Random Forest, f1: 0.381\n",
      "Model: Linear SVC, f1: 0.6858\n",
      "Model: KNN, f1: 0.506\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    if idx == 0:\n",
    "        print('Count Vectorizer')\n",
    "    else:\n",
    "        print('TF-IDF Vectorizer')\n",
    "\n",
    "    for i, model in enumerate(models_count):\n",
    "        count_v = vectorizers[idx].fit(X['tweets'])\n",
    "\n",
    "        # Transforma tus datos de entrenamiento y prueba utilizando el vectorizador \n",
    "        # CountVectorizer ajustado\n",
    "        X_train_count = count_v.transform(X['tweets'])\n",
    "        X_test_count = count_v.transform(x['tweets'])\n",
    "\n",
    "        # Ajusta el modelo PCA con los datos de entrenamiento\n",
    "        pca.fit(X_train_count.toarray())\n",
    "\n",
    "        # Transforma tus datos de entrenamiento y prueba utilizando el modelo PCA ajustado\n",
    "        X_train_pca = pca.transform(X_train_count.toarray())\n",
    "        X_test_pca = pca.transform(X_test_count.toarray())\n",
    "\n",
    "        model.fit(X_train_pca, y_train)\n",
    "        y_pred = model.predict(X_test_pca)\n",
    "        print(f'Model: {model_names[i]}, f1: {round(f1_score(y_test, y_pred, average=\"macro\"), 4)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramas y bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_count = [LogisticRegression(C=0.1, max_iter=1000, penalty='l2', random_state=42), \n",
    "          RandomForestClassifier(max_depth=50, n_estimators=10,random_state=42), \n",
    "          CalibratedClassifierCV(LinearSVC(C=0.1, max_iter=100000, penalty='l2', random_state=42)),  \n",
    "          KNeighborsClassifier(n_neighbors=3, weights='distance')]\n",
    "models_tfidf = [LogisticRegression(C=10.0, max_iter=1000, penalty='l2', random_state=42), \n",
    "          RandomForestClassifier(max_depth=100, n_estimators=10,random_state=42), \n",
    "          CalibratedClassifierCV(LinearSVC(C=1.0, max_iter=1000, penalty='l2', random_state=42)),  \n",
    "          KNeighborsClassifier(n_neighbors=3, weights='distance')]\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'Linear SVC', 'KNN']\n",
    "vectorizers = [CountVectorizer(ngram_range = (1,2), lowercase = False), \n",
    "              TfidfVectorizer(ngram_range = (1,2), lowercase = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "Model: Logistic Regression, f1: 0.6842\n",
      "Model: Random Forest, f1: 0.39\n",
      "Model: Linear SVC, f1: 0.6605\n",
      "Model: KNN, f1: 0.3924\n",
      "TF-IDF Vectorizer\n",
      "Model: Logistic Regression, f1: 0.3826\n",
      "Model: Random Forest, f1: 0.3445\n",
      "Model: Linear SVC, f1: 0.6054\n",
      "Model: KNN, f1: 0.4092\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    if idx == 0:\n",
    "        print('Count Vectorizer')\n",
    "    else:\n",
    "        print('TF-IDF Vectorizer')\n",
    "\n",
    "    for i, model in enumerate(models_count):\n",
    "        count_v = vectorizers[idx].fit(X['tweets'])\n",
    "\n",
    "        # Transforma tus datos de entrenamiento y prueba utilizando el vectorizador \n",
    "        # CountVectorizer ajustado\n",
    "        X_train_count = count_v.transform(X['tweets'])\n",
    "        X_test_count = count_v.transform(x['tweets'])\n",
    "\n",
    "        # Ajusta el modelo PCA con los datos de entrenamiento\n",
    "        pca.fit(X_train_count.toarray())\n",
    "\n",
    "        # Transforma tus datos de entrenamiento y prueba utilizando el modelo PCA ajustado\n",
    "        X_train_pca = pca.transform(X_train_count.toarray())\n",
    "        X_test_pca = pca.transform(X_test_count.toarray())\n",
    "\n",
    "        model.fit(X_train_pca, y_train)\n",
    "        y_pred = model.predict(X_test_pca)\n",
    "        print(f'Model: {model_names[i]}, f1: {round(f1_score(y_test, y_pred, average=\"macro\"), 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59d3b287212f50af165978b62192df5a49b6eea740d718d3743846fa9be15f35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
