{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data\n",
    "\n",
    "- `BOW`: Un diccionario que representa un \"Bag of Words\" (bolsa de palabras), que asocia palabras con sus respectivas frecuencias en un corpus.\n",
    "- `ULM`: Un diccionario que proporciona probabilidades iniciales de fondo (background) para cada palabra. Estas probabilidades suman 1 y representan un modelo base de cómo se distribuyen las palabras en general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW = {\n",
    "    'the' : 4,\n",
    "    'paper' : 2,\n",
    "    'text' : 4,\n",
    "    'mining' : 2\n",
    "}\n",
    "\n",
    "ULM = {\n",
    "    'the' : 0.5,\n",
    "    'paper' : 0.3,\n",
    "    'text' : 0.1,\n",
    "    'mining' : 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.3, 0.1, 0.1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convierte los valores del diccionario `ULM`` en un array de NumPy para cálculos posteriores.\n",
    "np.array(list(ULM.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999999999999999,\n",
       " [0.2764594829885556,\n",
       "  0.393871557703625,\n",
       "  0.1997951116104737,\n",
       "  0.12987384769734567])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa las probabilidades para los temas (topics) utilizando una distribución de Dirichlet.\n",
    "weight_initialization = np.random.dirichlet(np.ones(4),size=1)\n",
    "\n",
    "# La suma de todas las probabilidades generadas para asegurar que suman 1 (característica de la distribución de Dirichlet).\n",
    "sum(weight_initialization.flatten()), weight_initialization.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_algorithm(ulm, bow, p_topic_1, p_background, alpha):\n",
    "    \n",
    "\n",
    "    word_counts = np.array(list(bow.values()))\n",
    "    #initial_ps = np.random.dirichlet(np.ones(len(word_counts)), size= 1).flatten().tolist()\n",
    "    p_w_back = np.array(list(ulm.values()))\n",
    "    # M-step\n",
    "    p_w_topic = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    # E-step\n",
    "    p_z_word = (p_topic_1 * p_w_topic) / ((p_topic_1 * p_w_topic) + (p_background * p_w_back))\n",
    "    # Likelihood\n",
    "    likelihood = []\n",
    "    for i in range(len(word_counts)):\n",
    "        like = ((p_topic_1 * p_w_topic[i]) + (p_background * p_w_back[i])) ** word_counts[i]\n",
    "        likelihood.append(like)\n",
    "\n",
    "    log_like = np.log(np.prod(likelihood))\n",
    "\n",
    "    \n",
    "    lk = 99\n",
    "    it = 1\n",
    "    while lk >= alpha:\n",
    "        # M-step\n",
    "        new_p_w_topic = []\n",
    "        for i in range(len(word_counts)):\n",
    "            \n",
    "            new_p_w_t = (word_counts[i] * p_z_word[i]) / sum(word_counts * p_z_word)\n",
    "            new_p_w_topic.append(new_p_w_t)\n",
    "        \n",
    "        #E-step\n",
    "        new_p_z_word = (p_topic_1 * np.array(new_p_w_topic)) / ((p_topic_1 * np.array(new_p_w_topic)) + (p_background * p_w_back))\n",
    "\n",
    "        # Likelihood\n",
    "        new_likelihood = []\n",
    "        for i in range(len(word_counts)):\n",
    "            new_like = ((p_topic_1 * new_p_w_topic[i]) + (p_background * p_w_back[i])) ** word_counts[i]\n",
    "            new_likelihood.append(new_like)\n",
    "\n",
    "        new_log_like = np.log(np.prod(new_likelihood))\n",
    "        lk = np.absolute(log_like) - np.absolute(new_log_like)\n",
    "\n",
    "        p_w_topic = new_p_w_topic\n",
    "        p_z_word = new_p_z_word\n",
    "        log_like = new_log_like\n",
    "        it += 1\n",
    "    \n",
    "    print(it, lk)\n",
    "    return p_w_topic, p_z_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 9.818380137360805e-06\n"
     ]
    }
   ],
   "source": [
    "p_w_topic, p_z_word = em_algorithm(ULM, BOW, 0.5, 0.5, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.9033115393103048)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p_w_topic), sum(p_z_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
