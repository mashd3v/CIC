{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# División en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.4, random_state=42)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en subconjuntos con superposición\n",
    "def create_overlapping_subsets(X, y, subset_size=1500, overlap=500, num_subsets=11):\n",
    "    subsets_X = []\n",
    "    subsets_y = []\n",
    "    start = 0\n",
    "\n",
    "    for i in range(num_subsets):\n",
    "        end = start + subset_size if i < num_subsets - 1 else len(X)\n",
    "        subsets_X.append(X[start:end])\n",
    "        subsets_y.append(y[start:end])\n",
    "        start = end - overlap if i < num_subsets - 1 else len(X)\n",
    "    return subsets_X, subsets_y\n",
    "\n",
    "subsets_X_train, subsets_y_train = create_overlapping_subsets(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1307]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(subsets_X_train[i]) for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Crear la matriz TF-IDF para cada subconjunto\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=None)  # Sin limitar el número de características\n",
    "tfidf_global = tfidf_vectorizer.fit_transform(X_train)\n",
    "subsets_tfidf = [tfidf_vectorizer.transform(subset) for subset in subsets_X_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96465, 96465, 96465, 96465, 96465, 96465, 96465, 96465, 96465, 96465, 96465]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[subsets_tfidf[i].shape[1] for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import librería para simular SNN\n",
    "from brian2 import *\n",
    "\n",
    "start_scope()\n",
    "\n",
    "# Asumiendo que tfidf_matrix ya está definida y es accesible\n",
    "tfidf_matrix = subsets_tfidf[0]\n",
    "M = tfidf_matrix.shape[1]  # Número de neuronas en la primera capa, basado en la cantidad de características TF-IDF\n",
    "N = 1  # Neurona inhibitoria\n",
    "alpha = 1.5  # Coeficiente de proporcionalidad para las tasas de disparo\n",
    "\n",
    "# Parámetros del modelo LIF\n",
    "tau = 100*ms\n",
    "u_rest = -65*mV\n",
    "u_exc = 0*mV\n",
    "u_inh = -90*mV\n",
    "u_th = -52*mV\n",
    "t_ref = 3*ms\n",
    "tau_e = 2*ms\n",
    "tau_i = 2*ms\n",
    "\n",
    "# Modelo de neurona LIF\n",
    "eqs = '''\n",
    "                du/dt = ((u_rest - u) + g_e*(u_exc - u) + g_i*(u_inh - u)) / tau : volt (unless refractory)\n",
    "                dg_e/dt = -g_e / tau_e : 1\n",
    "                dg_i/dt = -g_i / tau_i : 1\n",
    "                '''\n",
    "\n",
    "# Creación de grupos de neuronas\n",
    "G = NeuronGroup(M, eqs, threshold='u > u_th', reset='u = u_rest', refractory=t_ref, method='euler')\n",
    "I = NeuronGroup(N, eqs, threshold='u > u_th', reset='u = u_rest', refractory=t_ref, method='euler')\n",
    "\n",
    "G.u = u_rest\n",
    "I.u = u_rest\n",
    "\n",
    "# Sinapsis\n",
    "S_ei = Synapses(G, I, on_pre='g_e += 0.2')\n",
    "S_ei.connect()\n",
    "\n",
    "S_ie = Synapses(I, G, on_pre='g_i += 0.5')\n",
    "S_ie.connect()\n",
    "\n",
    "# Corrección: Renombrar A_pre y R_post para evitar el error\n",
    "# Plasticidad STDP\n",
    "eqs_plasticity = '''\n",
    "                dA/dt = -A / tau_A : 1 (event-driven)\n",
    "                dR/dt = -R / tau_R : 1 (event-driven)\n",
    "                w : 1\n",
    "                '''\n",
    "on_pre = '''\n",
    "                A += 1\n",
    "                w = clip(w + (n * (A - (R + 0.1) * w)), 0, w_max)\n",
    "                '''\n",
    "on_post = '''\n",
    "                R += 1\n",
    "                '''\n",
    "\n",
    "tau_A = 5*ms  # Constante de tiempo de traza presináptica\n",
    "tau_R = 70*ms  # Constante de tiempo de traza postsináptica\n",
    "n = 0.01  # Constante de aprendizaje\n",
    "w_max = 1  # Peso máximo\n",
    "\n",
    "S_ee = Synapses(G, G, model=eqs_plasticity, on_pre=on_pre, on_post=on_post)\n",
    "S_ee.connect(condition='i != j')\n",
    "\n",
    "# Inicialización de pesos sinápticos con una distribución aleatoria\n",
    "w_init = 0.1  # Peso inicial máximo, ajusta según sea necesario para tu modelo\n",
    "S_ee.w = 'rand() * w_init'\n",
    "\n",
    "# Simulación para múltiples documentos\n",
    "n_documentos = len(tfidf_matrix)  # Número de documentos\n",
    "\n",
    "# Crear explícitamente un objeto Network\n",
    "net = Network()\n",
    "net.add(G)\n",
    "net.add(I)\n",
    "net.add(S_ei)\n",
    "net.add(S_ie)\n",
    "net.add(S_ee)\n",
    "\n",
    "# Ahora, dentro de tu bucle para procesar cada documento\n",
    "for doc_idx in range(n_documentos):\n",
    "    rates = tfidf_matrix[doc_idx] * alpha * Hz\n",
    "    P = PoissonGroup(M, rates=rates)\n",
    "    S_input = Synapses(P, G, on_pre='g_e += 0.1')\n",
    "    S_input.connect(j='i')\n",
    "    \n",
    "    # Agregar el grupo Poisson y las sinapsis a la red para este documento\n",
    "    net.add(P)\n",
    "    net.add(S_input)\n",
    "    \n",
    "    # Monitores (recreados para cada documento para evitar MagicError)\n",
    "    M_spikes = SpikeMonitor(G)\n",
    "    M_v = StateMonitor(G, 'v', record=True)\n",
    "    \n",
    "    # Agregar los monitores a la red\n",
    "    net.add(M_spikes)\n",
    "    net.add(M_v)\n",
    "    \n",
    "    # Ejecutar la simulación para este documento\n",
    "    net.run(600*ms)\n",
    "\n",
    "    # Visualización\n",
    "    figure(figsize=(12, 6))\n",
    "    subplot(121)\n",
    "    plot(M_spikes.t/ms, M_spikes.i, '.k')\n",
    "    xlim(0, 600)\n",
    "    xlabel('Time (ms)')\n",
    "    ylabel('Neuron index')\n",
    "    \n",
    "    subplot(122)\n",
    "    plot(M_v.t/ms, M_v.u[0]/mV)\n",
    "    xlabel('Time (ms)')\n",
    "    ylabel('Membrane potential (mV)')\n",
    "    \n",
    "    show()\n",
    "\n",
    "    # Importante: borra los monitores y el grupo Poisson de la red antes de la próxima iteración\n",
    "    net.remove(M_spikes)\n",
    "    net.remove(M_v)\n",
    "    net.remove(P)\n",
    "    net.remove(S_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
